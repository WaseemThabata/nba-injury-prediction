# Results Directory

This directory contains model evaluation outputs, visualizations, and performance metrics generated by the pipeline.

## Generated Files

After running `python scripts/run_pipeline.py` or `python scripts/demo_synthetic.py`, this directory will contain:

### 1. metrics.json
**Description**: Performance metrics in JSON format

**Contents**:
```json
{
  "balanced_accuracy": 0.6009,
  "accuracy": 0.6500,
  "auc": 0.6800,
  "threshold": 0.2,
  "confusion_matrix": {
    "TN": 184,
    "FP": 110,
    "FN": 31,
    "TP": 33
  }
}
```

**Metrics Explained**:
- **Balanced Accuracy**: Average of recall across classes (compensates for class imbalance)
- **Accuracy**: Overall classification accuracy
- **AUC**: Area Under ROC Curve (model's ability to distinguish classes)
- **Threshold**: Probability cutoff for classification (0.2 from R analysis)
- **Confusion Matrix**:
  - TN (True Negatives): Correctly predicted no injury
  - FP (False Positives): Predicted injury but no injury occurred
  - FN (False Negatives): Predicted no injury but injury occurred
  - TP (True Positives): Correctly predicted injury

### 2. confusion_matrix.png
**Description**: Heatmap visualization of confusion matrix

**Interpretation**:
- Darker blue = Higher count
- Diagonal (top-left to bottom-right) = Correct predictions
- Off-diagonal = Misclassifications

### 3. feature_importance.png
**Description**: Top 10 most important features for prediction

**Interpretation**:
- Features ranked by "gain" (improvement in accuracy they provide)
- Higher gain = More important for predictions
- Helps identify key injury risk factors

**Expected Top Features** (from R analysis):
1. Days since last injury
2. Age-adjusted usage rate
3. Rolling 30-day workload
4. Career injury count
5. Position-specific risk

### 4. shap_summary.png
**Description**: SHAP beeswarm plot showing feature effects

**Interpretation**:
- Each dot = One prediction
- Horizontal position = SHAP value (effect on prediction)
- Color = Feature value (red=high, blue=low)
- Features ordered by importance (top to bottom)

**How to Read**:
- Red dots on right = High feature value increases injury risk
- Blue dots on left = Low feature value decreases injury risk
- Wide spread = Feature has variable effects across samples

### 5. shap_importance.png
**Description**: SHAP bar chart of mean absolute impacts

**Interpretation**:
- Simple ranking of features by average importance
- Complements the summary plot
- Useful for quick feature prioritization

## Usage Examples

### Load Metrics Programmatically
```python
import json

with open('results/metrics.json', 'r') as f:
    metrics = json.load(f)

print(f"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}")
print(f"AUC: {metrics['auc']:.4f}")
```

### Display Images
```python
import matplotlib.pyplot as plt
from PIL import Image

img = Image.open('results/confusion_matrix.png')
plt.imshow(img)
plt.axis('off')
plt.show()
```

### In Streamlit App
```python
import streamlit as st

st.image('results/feature_importance.png', 
         caption='Feature Importance')
```

## File Sizes

Typical file sizes (may vary):
- `metrics.json`: ~200 bytes
- `confusion_matrix.png`: ~90 KB
- `feature_importance.png`: ~80 KB
- `shap_summary.png`: ~350 KB
- `shap_importance.png`: ~190 KB

Total: ~700 KB

## Regenerating Results

To regenerate all results:

```bash
# With real data
python scripts/run_pipeline.py

# With synthetic data (demo)
python scripts/demo_synthetic.py

# Just regenerate visualizations from existing model
python scripts/generate_results.py
```

## Interpreting Results

### Good Model Performance Indicators
- ✅ Balanced accuracy > 0.55 (better than random for imbalanced data)
- ✅ AUC > 0.60 (reasonable discrimination)
- ✅ Confusion matrix with high TN and TP
- ✅ Feature importance matches domain knowledge

### What the R Analysis Achieved
- Balanced Accuracy: **0.6009** (60.09%)
- Threshold: **0.2** (optimized for recall)
- Train: 2010-2018, Test: 2019-2020

### Expected Python Results
With same hyperparameters and data, Python should achieve:
- Balanced Accuracy: ~0.60 (±0.01)
- Similar AUC and confusion matrix
- Same feature importance rankings

## Notes

- Results are deterministic (fixed random seed: 111111)
- Threshold of 0.2 favors recall (catching injuries)
- Class imbalance means accuracy alone is misleading
- SHAP analysis may take 1-5 minutes to compute

## Troubleshooting

**Missing files?**
- Make sure you've run the pipeline first
- Check that data files exist in `data/raw/`

**Different results than R?**
- Verify same data preprocessing
- Check hyperparameters in `config/config.yaml`
- Ensure same train/test split years

**Low performance?**
- Try demo with synthetic data first
- Check data quality and missing values
- Review feature engineering in preprocessing

---

**Last Updated**: December 2024  
**Version**: 1.0.0
